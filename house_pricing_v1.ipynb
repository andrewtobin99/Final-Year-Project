{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Competition Notebook Template\n",
    "\n",
    "### Student Name : \n",
    "\n",
    "**Name Surname ID**  \n",
    "\n",
    "\n",
    "#### Summary\n",
    "Please write here the summary of your implementation(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import plotly as pl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import patsy as patsy\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import functions as f\n",
    "\n",
    "dftrain = pd.read_csv(\"/Users/andrewtobin/College/final_year/FYP/House_pricing_dataset-master/dataset_csv/train.csv\",index_col='ad_id')\n",
    "dftest  = pd.read_csv( \"/Users/andrewtobin/College/final_year/FYP/House_pricing_dataset-master/dataset_csv/test.csv\",index_col='ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Rows:  2385\n",
      "Test Data Rows:  597\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(dftrain, test_size=0.2)\n",
    "\n",
    "print('Train Data Rows: ',len(train_data))\n",
    "\n",
    "print('Test Data Rows: ',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning, Features selection and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_facility(df):\n",
    "    split_df = df['facility'].str.split(',', expand = True) # splits the unstructured lists\n",
    "    facility_dummy = pd.get_dummies(split_df[[0,1,2,3,4]].apply(pd.Series), prefix = '') # creates dummies from unstructured lists (duplicated columns)\n",
    "\n",
    "    def sjoin(x): return ';'.join(x[x.notnull()].astype(str)) \n",
    "    facility_dummy = facility_dummy.groupby(level=0, axis=1).apply(lambda x: x.apply(sjoin, axis=1)) ## joins columns but keeps multiple values i.e. 0;0;0;0\n",
    "\n",
    "    for column in facility_dummy: ## workaround such that only one value is kept \n",
    "        facility_dummy[column] = facility_dummy[column].str.replace(\";\", \"\") ## removes the separator ';'\n",
    "        facility_dummy[column] = facility_dummy[column].astype('int')\n",
    "\n",
    "    facility_dummy[facility_dummy >= 1] = 1\n",
    "    return facility_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_data(df):\n",
    "    ## drop unnecessary columns\n",
    "    df = df.drop(['county', 'environment', 'description_block', 'features'], axis=1)\n",
    "\n",
    "    ## correct ber exempt value in ber_classification column\n",
    "    df['ber_classification'] = df['ber_classification'].str.replace('SINo666of2006exempt', 'BER Exempt')\n",
    "\n",
    "    ## create a log transform column for price to cut down the variance\n",
    "    df['log_price'] = df.price.apply(lambda x: math.log(x))\n",
    "\n",
    "    ## dropping rows where price/beds/baths is null for now for now (we might look at imputing these missing values later)\n",
    "    df = df[df['price'].notnull()]\n",
    "    df =df[df['beds'].notnull()]\n",
    "    df =df[df['beds'].notnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(df):\n",
    "    area_dummy = pd.get_dummies(df['area'], prefix='area')\n",
    "    ber_dummy = pd.get_dummies(df['ber_classification'], prefix='ber')\n",
    "    property_type_dummy = pd.get_dummies(df['property_type'], prefix='property_type')\n",
    "\n",
    "    dummy_matrix = pd.concat([area_dummy, ber_dummy, property_type_dummy], axis=1)\n",
    "\n",
    "    return dummy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(df, x, model):\n",
    "    ## makes a prediction for the log price\n",
    "    df['log_predicted_price'] = model.predict(x.astype(float))\n",
    "    \n",
    "\n",
    "    ## gets the exponent of the log price to arrive at final predicted price\n",
    "    df['predicted_price'] = np.exp(df['log_predicted_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prep_data(train_data)\n",
    "test_data = prep_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put together full dummy matrix for all categorical variables\n",
    "train_dummy = get_dummies(train_data)\n",
    "fac_dummy = split_facility(train_data)\n",
    "train_dummy = pd.concat([train_dummy, fac_dummy], axis=1)\n",
    "\n",
    "## do the same for test data\n",
    "test_dummy = get_dummies(test_data)\n",
    "fac_dummy2 = split_facility(test_data)\n",
    "test_dummy = pd.concat([test_dummy, fac_dummy2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([train_dummy, train_data['beds'], train_data['bathrooms'], train_data['longitude'], train_data['latitude'], train_data['surface']], axis=1)\n",
    "Y_train = train_data['log_price']\n",
    "\n",
    "X_test = pd.concat([test_dummy, test_data['beds'], test_data['bathrooms'], test_data['longitude'], test_data['latitude'], test_data['surface']], axis=1)\n",
    "Y_test = test_data['log_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2313, 187)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## checking for NaNs (Found 395 rows with missing value for Surface Area)\n",
    "#for col in X_train:\n",
    "    #print(col, ': ', X_train[col].isnull().sum())\n",
    "\n",
    "print(Y_train.isnull().sum())\n",
    "\n",
    "#for col in X_test:\n",
    "    #print(col, ': ', X_test[col].isnull().sum())\n",
    "\n",
    "print(Y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to accommodate linear regression, where surface == NaN we will set value to average of surface\n",
    "X_train['surface'] = X_train['surface'].fillna((X_train['surface'].mean()))\n",
    "\n",
    "X_test['surface'] = X_test['surface'].fillna((X_test['surface'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, Y_train)\n",
    "# predictions for training data\n",
    "make_predictions(train_data, X_train, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute MdAPE: Median absolute percentage error (less sensitive to outliers than MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MdAPE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return round(np.median(np.abs((y_true - y_pred) / y_true)) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true,y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MdAPE:  12.48 %\n",
      "Training MAPE:  18.82 %\n"
     ]
    }
   ],
   "source": [
    "print('Training MdAPE: ', MdAPE(train_data['price'], train_data['predicted_price']), '%')\n",
    "print('Training MAPE: ', MAPE(train_data['price'], train_data['predicted_price']), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([8.43234626e+04, 8.85261028e+01, 3.62607542e+01, 3.05536039e+01,\n       2.25146307e+01, 2.09095391e+01, 1.72802752e+01, 1.49353055e+01,\n       1.47077879e+01, 1.44537257e+01, 1.42220228e+01, 1.35621198e+01,\n       1.32955567e+01, 1.24123840e+01, 1.21799587e+01, 1.19182693e+01,\n       1.17483988e+01, 1.16594365e+01, 1.14596985e+01, 1.00208943e+01,\n       8.90765921e+00, 8.73268039e+00, 8.39044386e+00, 8.16649739e+00,\n       8.03168097e+00, 7.83803454e+00, 7.73815482e+00, 7.61109270e+00,\n       7.46758655e+00, 7.29789114e+00, 7.29029897e+00, 7.21546776e+00,\n       7.05701712e+00, 7.01716953e+00, 6.95452686e+00, 6.87804884e+00,\n       6.76782420e+00, 6.70969710e+00, 6.58781330e+00, 6.50078892e+00,\n       6.27845787e+00, 6.18588129e+00, 5.85890063e+00, 5.75570847e+00,\n       5.65880117e+00, 5.60787511e+00, 5.56431956e+00, 5.48657167e+00,\n       5.42165800e+00, 5.36919121e+00, 5.32321994e+00, 5.27190092e+00,\n       5.24135887e+00, 5.09642177e+00, 5.07163804e+00, 5.02755989e+00,\n       5.01246018e+00, 4.89812607e+00, 4.89314052e+00, 4.75325029e+00,\n       4.72800186e+00, 4.69531754e+00, 4.63991226e+00, 4.58072006e+00,\n       4.51374479e+00, 4.40067639e+00, 4.38888108e+00, 4.33640735e+00,\n       4.31390922e+00, 4.29436911e+00, 4.26611666e+00, 4.23252350e+00,\n       4.20091998e+00, 4.15700301e+00, 4.15456332e+00, 4.08187843e+00,\n       4.00097368e+00, 3.91999824e+00, 3.87172555e+00, 3.86127238e+00,\n       3.84999229e+00, 3.80670607e+00, 3.77615477e+00, 3.75315311e+00,\n       3.73830132e+00, 3.72959149e+00, 3.69991258e+00, 3.65089695e+00,\n       3.64720910e+00, 3.60935603e+00, 3.56381032e+00, 3.53797035e+00,\n       3.49975148e+00, 3.46252040e+00, 3.43637851e+00, 3.42833959e+00,\n       3.42266467e+00, 3.41366839e+00, 3.36235600e+00, 3.33180355e+00,\n       3.29874562e+00, 3.28737767e+00, 3.27494082e+00, 3.25294286e+00,\n       3.23104155e+00, 3.17598682e+00, 3.15938567e+00, 3.12425396e+00,\n       3.09682788e+00, 3.05887760e+00, 3.02395183e+00, 2.99471333e+00,\n       2.98792287e+00, 2.97515635e+00, 2.97258981e+00, 2.96316067e+00,\n       2.95901744e+00, 2.92506659e+00, 2.83077183e+00, 2.81334859e+00,\n       2.78710266e+00, 2.77432076e+00, 2.67096348e+00, 2.63683764e+00,\n       2.63655177e+00, 2.62431485e+00, 2.58933226e+00, 2.56980700e+00,\n       2.52561045e+00, 2.46983566e+00, 2.44113422e+00, 2.43795100e+00,\n       2.43248161e+00, 2.42461130e+00, 2.42008211e+00, 2.39776468e+00,\n       2.24762397e+00, 2.23784469e+00, 2.22834484e+00, 2.22342330e+00,\n       2.21827532e+00, 2.20476683e+00, 2.20015963e+00, 2.01779368e+00,\n       2.01169306e+00, 1.99907789e+00, 1.97394970e+00, 1.96173460e+00,\n       1.77420531e+00, 1.75865359e+00, 1.74409325e+00, 1.72976117e+00,\n       1.72783911e+00, 1.72553412e+00, 1.72049559e+00, 1.71963330e+00,\n       1.71360064e+00, 1.71134074e+00, 1.70491227e+00, 1.43554070e+00,\n       1.43471996e+00, 1.41243965e+00, 1.40918540e+00, 1.40792248e+00,\n       1.40345156e+00, 1.39512446e+00, 1.38281331e+00, 1.05799167e+00,\n       1.01415641e+00, 1.00433595e+00, 9.99466922e-01, 9.99053774e-01,\n       9.98031436e-01, 9.95491603e-01, 9.95183395e-01, 9.94827213e-01,\n       9.93004790e-01, 9.91461888e-01, 9.90009236e-01, 9.87083171e-01,\n       9.85513461e-01, 9.69364795e-01, 9.50548256e-01, 3.79561042e-01,\n       2.43971178e-01, 2.99577119e-10, 1.86045615e-14])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importance = reg.coef_\n",
    "# summarize feature importance\n",
    "#for i,v in enumerate(importance):\n",
    "\t#print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "#plt.bar([x for x in range(len(importance))], importance)\n",
    "#plt.show()\n",
    "#importance = -np.sort(-importance)\n",
    "#top_50 = importance[0:50]\n",
    "#for i, v in enumerate(top_50):\n",
    "#\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "#plt.bar([x for x in range(len(top_50))], top_50)\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}